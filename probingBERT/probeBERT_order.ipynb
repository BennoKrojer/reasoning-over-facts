{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "probeBERT_order.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCg_nBdn1HVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0JUhsiM5k4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cased_model = True\n",
        "model_type = 'roberta'\n",
        "mask_token = '[MASK]' if model_type == 'bert' else '<mask>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_q27yxX1YtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "order = ['The Bible', 'The Divine Comedy', 'The Wealth of Nations', 'Faust', 'Moby Dick', 'Brave New World', 'To Kill a Mockingbird', 'Harry Potter']\n",
        "# order = ['Jimmy Carter', 'Donald Trump', 'Barack Obama', 'Tiger Woods', 'Serena Williams', 'Ariana Grande', 'Kylie Jenner']\n",
        "# order = ['falcon', 'cheetah', 'swordfish','antelope', 'lion', 'kangaroo', 'dog', 'pig', 'cow', 'hedgehog', 'snail']\n",
        "# order = ['Ferrari', 'Porsche', 'Audi', 'VW', 'bike']\n",
        "# order = ['the Sun', 'Jupiter', 'Saturn', 'Uranus', 'Neptune', 'Earth', 'Venus', 'Mars', 'Mercury' ,'Pluto']\n",
        "#ordered by population:\n",
        "# order = ['China', 'Indonesia', 'Brazil', 'Japan', 'Egypt', 'Germany', 'Italy', 'Argentina', 'Australia', 'Chile', 'Belgium', 'Sweden', 'Denmark', 'Ireland', 'Slovenia', 'Malta']\n",
        "# order = ['New York City', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'San Jose', 'Austin', 'San Francisco', 'Seattle', 'Boston', 'Detroit', 'Portland', 'Las Vegas', 'Atlanta', 'Miami', 'New Orleans']\n",
        "# order = ['nail', 'pen', 'laptop', 'table', 'house', 'airplane', 'city', 'sun'] #reproduce olmpics\n",
        "# order = ['Russia', 'Canada', 'China', 'Brazil','Australia', 'India', 'Argentina', 'Kazakhstan', 'Algeria', 'Saudi Arabia', 'Mexico', 'Indonesia', 'Turkey', 'France', 'Italy', 'Ireland', 'Belgium', 'Monaco']\n",
        "# bigger = ['faster']\n",
        "# smaller = ['slower']\n",
        "# bigger = ['bigger', 'larger', 'greater', 'more', 'older']\n",
        "# smaller = ['smaller', 'less', 'younger']\n",
        "# bigger = ['older']\n",
        "# smaller = ['younger']\n",
        "bigger = ['before']\n",
        "smaller = ['after']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSfjnyfN2edH",
        "colab_type": "code",
        "outputId": "924afde4-1513-4887-9c87-1c36eaa67d61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "pairs = []\n",
        "for i in range(len(order)):\n",
        "  for j in range(i+1, len(order)):\n",
        "    e1, e2 = order[i], order[j]\n",
        "    prefix = ''\n",
        "    probe = f'{prefix}{e1} was published {mask_token} {prefix}{e2} .'\n",
        "    # probe = '[CLS] ' + probe + ' [SEP]'\n",
        "    rev_probe = f'{prefix}{e2} was published {mask_token} {prefix}{e1} .'\n",
        "    # rev_probe = '[CLS] ' + rev_probe + ' [SEP]'\n",
        "    probe, rev_probe = probe[0].upper() + probe[1:], rev_probe[0].upper() + rev_probe[1:]\n",
        "    pairs.append(((order[i], order[j]), (probe, rev_probe)))\n",
        "pairs[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('The Bible', 'The Divine Comedy'),\n",
              "  ('The Bible was published <mask> The Divine Comedy .',\n",
              "   'The Divine Comedy was published <mask> The Bible .')),\n",
              " (('The Bible', 'The Wealth of Nations'),\n",
              "  ('The Bible was published <mask> The Wealth of Nations .',\n",
              "   'The Wealth of Nations was published <mask> The Bible .')),\n",
              " (('The Bible', 'Faust'),\n",
              "  ('The Bible was published <mask> Faust .',\n",
              "   'Faust was published <mask> The Bible .')),\n",
              " (('The Bible', 'Moby Dick'),\n",
              "  ('The Bible was published <mask> Moby Dick .',\n",
              "   'Moby Dick was published <mask> The Bible .')),\n",
              " (('The Bible', 'Brave New World'),\n",
              "  ('The Bible was published <mask> Brave New World .',\n",
              "   'Brave New World was published <mask> The Bible .'))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xm2SbTa1ZMm",
        "colab_type": "code",
        "outputId": "d6e3f579-1b06-44ac-e9f0-e86f30290546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM, RobertaForMaskedLM, RobertaTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# from: https://huggingface.co/transformers/quickstart.html#bert-example\n",
        "if model_type == 'bert':\n",
        "  bert_model = 'bert-large-cased' if cased_model else 'bert-large-uncased'\n",
        "  tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
        "  model = BertForMaskedLM.from_pretrained(bert_model)\n",
        "elif model_type == 'roberta':\n",
        "  tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
        "  model = RobertaForMaskedLM.from_pretrained('roberta-large')\n",
        "model.eval()\n",
        "print(f'{model_type} ready')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "roberta ready\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcAoeH_lS-wH",
        "colab_type": "code",
        "outputId": "1f1e0d52-ae03-42a7-d8e6-a664b7faa0d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "all_adj = bigger + smaller\n",
        "proc_adj = ['Ġ'+adj for adj in all_adj] if model_type == 'roberta' else all_adj\n",
        "ids = tokenizer.convert_tokens_to_ids(proc_adj)\n",
        "adj2id = {adj:i for adj, i in zip(all_adj, ids)}\n",
        "print(adj2id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'before': 137, 'after': 71}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni2lLPdm4vfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, tokenized_text):\n",
        "  if model_type == 'bert':\n",
        "    print(tokenized_text)\n",
        "    masked_index = [i for i, x in enumerate(tokenized_text) if x == '[MASK]'][0]\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    segments_ids = [0]*len(tokenized_text)\n",
        "\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "    predictions = outputs[0][0][masked_index]\n",
        "    predicted_ids = torch.argsort(predictions, descending=True)[:5]\n",
        "    top_tokens = tokenizer.convert_ids_to_tokens(predicted_ids)\n",
        "    \n",
        "  \n",
        "  elif model_type == 'roberta':\n",
        "    print(tokenized_text)\n",
        "    masked_index = tokenized_text.index('<mask>')+1\n",
        "    input_ids = torch.tensor(tokenizer.encode(tokenized_text,add_special_tokens=True)).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "      outputs = model(input_ids, masked_lm_labels=input_ids)\n",
        "    loss, prediction_scores = outputs[:2]\n",
        "    predictions = prediction_scores[0, masked_index]\n",
        "    predicted_k_indexes = torch.topk(prediction_scores[0, masked_index],k=5)\n",
        "    predicted_indexes_list = predicted_k_indexes[1]\n",
        "    top_tokens = [tokenizer.decode(i) for i in predicted_indexes_list.tolist()]\n",
        "  return predictions, top_tokens\n",
        "\n",
        "def top_adj(preds):\n",
        "  top_val = 0\n",
        "  top_adj = ''\n",
        "  for adj, i in adj2id.items():\n",
        "    if preds[i] > top_val:\n",
        "      top_val = preds[i]\n",
        "      top_adj = adj\n",
        "  return top_adj"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O2wZzH0cmFf",
        "colab_type": "code",
        "outputId": "1eca42da-0613-4e46-831e-a9930e9504fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "output = {'consistent_correct': [], 'consistent_incorrect': [],'inconsistent':[]}\n",
        "for i, ((e1, e2), (probe, rev_probe)) in enumerate(pairs):\n",
        "  print(f'pair {i}')\n",
        "  mask_preds1, top_token1 = predict(model, tokenizer.tokenize(probe))\n",
        "  mask_preds2, top_token2 = predict(model, tokenizer.tokenize(rev_probe))\n",
        "  token_pred1 = bigger[0] if top_adj(mask_preds1) in bigger else smaller[0]\n",
        "  token_pred2 = bigger[0] if top_adj(mask_preds2) in bigger else smaller[0]\n",
        "  result = ((probe.replace(mask_token, token_pred1.upper()), top_token1), (rev_probe.replace(mask_token, token_pred2.upper()), top_token2))\n",
        "  if (token_pred1 == token_pred2):\n",
        "    output['inconsistent'].append(result)\n",
        "  elif token_pred1 == bigger[0]:\n",
        "    output['consistent_correct'].append(result)\n",
        "  else:\n",
        "    output['consistent_incorrect'].append(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pair 0\n",
            "['The', 'ĠBible', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠDivine', 'ĠComedy', 'Ġ.']\n",
            "['The', 'ĠDivine', 'ĠComedy', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠBible', 'Ġ.']\n",
            "pair 1\n",
            "['The', 'ĠBible', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠWealth', 'Ġof', 'ĠNations', 'Ġ.']\n",
            "['The', 'ĠWealth', 'Ġof', 'ĠNations', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠBible', 'Ġ.']\n",
            "pair 2\n",
            "['The', 'ĠBible', 'Ġwas', 'Ġpublished', '<mask>', 'ĠFaust', 'Ġ.']\n",
            "['Fa', 'ust', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠBible', 'Ġ.']\n",
            "pair 3\n",
            "['The', 'ĠBible', 'Ġwas', 'Ġpublished', '<mask>', 'ĠMob', 'y', 'ĠDick', 'Ġ.']\n",
            "['M', 'oby', 'ĠDick', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠBible', 'Ġ.']\n",
            "pair 4\n",
            "['The', 'ĠBible', 'Ġwas', 'Ġpublished', '<mask>', 'ĠBrave', 'ĠNew', 'ĠWorld', 'Ġ.']\n",
            "['Brave', 'ĠNew', 'ĠWorld', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠBible', 'Ġ.']\n",
            "pair 5\n",
            "['The', 'ĠBible', 'Ġwas', 'Ġpublished', '<mask>', 'ĠTo', 'ĠKill', 'Ġa', 'ĠM', 'ocking', 'bird', 'Ġ.']\n",
            "['To', 'ĠKill', 'Ġa', 'ĠM', 'ocking', 'bird', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠBible', 'Ġ.']\n",
            "pair 6\n",
            "['The', 'ĠBible', 'Ġwas', 'Ġpublished', '<mask>', 'ĠHarry', 'ĠPotter', 'Ġ.']\n",
            "['Harry', 'ĠPotter', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠBible', 'Ġ.']\n",
            "pair 7\n",
            "['The', 'ĠDivine', 'ĠComedy', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠWealth', 'Ġof', 'ĠNations', 'Ġ.']\n",
            "['The', 'ĠWealth', 'Ġof', 'ĠNations', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠDivine', 'ĠComedy', 'Ġ.']\n",
            "pair 8\n",
            "['The', 'ĠDivine', 'ĠComedy', 'Ġwas', 'Ġpublished', '<mask>', 'ĠFaust', 'Ġ.']\n",
            "['Fa', 'ust', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠDivine', 'ĠComedy', 'Ġ.']\n",
            "pair 9\n",
            "['The', 'ĠDivine', 'ĠComedy', 'Ġwas', 'Ġpublished', '<mask>', 'ĠMob', 'y', 'ĠDick', 'Ġ.']\n",
            "['M', 'oby', 'ĠDick', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠDivine', 'ĠComedy', 'Ġ.']\n",
            "pair 10\n",
            "['The', 'ĠDivine', 'ĠComedy', 'Ġwas', 'Ġpublished', '<mask>', 'ĠBrave', 'ĠNew', 'ĠWorld', 'Ġ.']\n",
            "['Brave', 'ĠNew', 'ĠWorld', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠDivine', 'ĠComedy', 'Ġ.']\n",
            "pair 11\n",
            "['The', 'ĠDivine', 'ĠComedy', 'Ġwas', 'Ġpublished', '<mask>', 'ĠTo', 'ĠKill', 'Ġa', 'ĠM', 'ocking', 'bird', 'Ġ.']\n",
            "['To', 'ĠKill', 'Ġa', 'ĠM', 'ocking', 'bird', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠDivine', 'ĠComedy', 'Ġ.']\n",
            "pair 12\n",
            "['The', 'ĠDivine', 'ĠComedy', 'Ġwas', 'Ġpublished', '<mask>', 'ĠHarry', 'ĠPotter', 'Ġ.']\n",
            "['Harry', 'ĠPotter', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠDivine', 'ĠComedy', 'Ġ.']\n",
            "pair 13\n",
            "['The', 'ĠWealth', 'Ġof', 'ĠNations', 'Ġwas', 'Ġpublished', '<mask>', 'ĠFaust', 'Ġ.']\n",
            "['Fa', 'ust', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠWealth', 'Ġof', 'ĠNations', 'Ġ.']\n",
            "pair 14\n",
            "['The', 'ĠWealth', 'Ġof', 'ĠNations', 'Ġwas', 'Ġpublished', '<mask>', 'ĠMob', 'y', 'ĠDick', 'Ġ.']\n",
            "['M', 'oby', 'ĠDick', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠWealth', 'Ġof', 'ĠNations', 'Ġ.']\n",
            "pair 15\n",
            "['The', 'ĠWealth', 'Ġof', 'ĠNations', 'Ġwas', 'Ġpublished', '<mask>', 'ĠBrave', 'ĠNew', 'ĠWorld', 'Ġ.']\n",
            "['Brave', 'ĠNew', 'ĠWorld', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠWealth', 'Ġof', 'ĠNations', 'Ġ.']\n",
            "pair 16\n",
            "['The', 'ĠWealth', 'Ġof', 'ĠNations', 'Ġwas', 'Ġpublished', '<mask>', 'ĠTo', 'ĠKill', 'Ġa', 'ĠM', 'ocking', 'bird', 'Ġ.']\n",
            "['To', 'ĠKill', 'Ġa', 'ĠM', 'ocking', 'bird', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠWealth', 'Ġof', 'ĠNations', 'Ġ.']\n",
            "pair 17\n",
            "['The', 'ĠWealth', 'Ġof', 'ĠNations', 'Ġwas', 'Ġpublished', '<mask>', 'ĠHarry', 'ĠPotter', 'Ġ.']\n",
            "['Harry', 'ĠPotter', 'Ġwas', 'Ġpublished', '<mask>', 'ĠThe', 'ĠWealth', 'Ġof', 'ĠNations', 'Ġ.']\n",
            "pair 18\n",
            "['Fa', 'ust', 'Ġwas', 'Ġpublished', '<mask>', 'ĠMob', 'y', 'ĠDick', 'Ġ.']\n",
            "['M', 'oby', 'ĠDick', 'Ġwas', 'Ġpublished', '<mask>', 'ĠFaust', 'Ġ.']\n",
            "pair 19\n",
            "['Fa', 'ust', 'Ġwas', 'Ġpublished', '<mask>', 'ĠBrave', 'ĠNew', 'ĠWorld', 'Ġ.']\n",
            "['Brave', 'ĠNew', 'ĠWorld', 'Ġwas', 'Ġpublished', '<mask>', 'ĠFaust', 'Ġ.']\n",
            "pair 20\n",
            "['Fa', 'ust', 'Ġwas', 'Ġpublished', '<mask>', 'ĠTo', 'ĠKill', 'Ġa', 'ĠM', 'ocking', 'bird', 'Ġ.']\n",
            "['To', 'ĠKill', 'Ġa', 'ĠM', 'ocking', 'bird', 'Ġwas', 'Ġpublished', '<mask>', 'ĠFaust', 'Ġ.']\n",
            "pair 21\n",
            "['Fa', 'ust', 'Ġwas', 'Ġpublished', '<mask>', 'ĠHarry', 'ĠPotter', 'Ġ.']\n",
            "['Harry', 'ĠPotter', 'Ġwas', 'Ġpublished', '<mask>', 'ĠFaust', 'Ġ.']\n",
            "pair 22\n",
            "['M', 'oby', 'ĠDick', 'Ġwas', 'Ġpublished', '<mask>', 'ĠBrave', 'ĠNew', 'ĠWorld', 'Ġ.']\n",
            "['Brave', 'ĠNew', 'ĠWorld', 'Ġwas', 'Ġpublished', '<mask>', 'ĠMob', 'y', 'ĠDick', 'Ġ.']\n",
            "pair 23\n",
            "['M', 'oby', 'ĠDick', 'Ġwas', 'Ġpublished', '<mask>', 'ĠTo', 'ĠKill', 'Ġa', 'ĠM', 'ocking', 'bird', 'Ġ.']\n",
            "['To', 'ĠKill', 'Ġa', 'ĠM', 'ocking', 'bird', 'Ġwas', 'Ġpublished', '<mask>', 'ĠMob', 'y', 'ĠDick', 'Ġ.']\n",
            "pair 24\n",
            "['M', 'oby', 'ĠDick', 'Ġwas', 'Ġpublished', '<mask>', 'ĠHarry', 'ĠPotter', 'Ġ.']\n",
            "['Harry', 'ĠPotter', 'Ġwas', 'Ġpublished', '<mask>', 'ĠMob', 'y', 'ĠDick', 'Ġ.']\n",
            "pair 25\n",
            "['Brave', 'ĠNew', 'ĠWorld', 'Ġwas', 'Ġpublished', '<mask>', 'ĠTo', 'ĠKill', 'Ġa', 'ĠM', 'ocking', 'bird', 'Ġ.']\n",
            "['To', 'ĠKill', 'Ġa', 'ĠM', 'ocking', 'bird', 'Ġwas', 'Ġpublished', '<mask>', 'ĠBrave', 'ĠNew', 'ĠWorld', 'Ġ.']\n",
            "pair 26\n",
            "['Brave', 'ĠNew', 'ĠWorld', 'Ġwas', 'Ġpublished', '<mask>', 'ĠHarry', 'ĠPotter', 'Ġ.']\n",
            "['Harry', 'ĠPotter', 'Ġwas', 'Ġpublished', '<mask>', 'ĠBrave', 'ĠNew', 'ĠWorld', 'Ġ.']\n",
            "pair 27\n",
            "['To', 'ĠKill', 'Ġa', 'ĠM', 'ocking', 'bird', 'Ġwas', 'Ġpublished', '<mask>', 'ĠHarry', 'ĠPotter', 'Ġ.']\n",
            "['Harry', 'ĠPotter', 'Ġwas', 'Ġpublished', '<mask>', 'ĠTo', 'ĠKill', 'Ġa', 'ĠM', 'ocking', 'bird', 'Ġ.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlw20MSrWrdC",
        "colab_type": "code",
        "outputId": "4c4a0334-31e0-427d-e442-770ffdf6fc3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print([(key, len(output[key]))for key in output])\n",
        "for key, val in output.items():\n",
        "  print(key + ' ' + str(len(val)))\n",
        "  for (probe, pred), (rev_probe, rev_pred) in val:\n",
        "    print(f'{probe} → {pred}')\n",
        "    print(f'{rev_probe} → {rev_pred}\\n')\n",
        "\n",
        "  print('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('consistent_correct', 5), ('consistent_incorrect', 3), ('inconsistent', 20)]\n",
            "consistent_correct 5\n",
            "The Divine Comedy was published BEFORE The Wealth of Nations . → [' in', ' as', ' by', ' with', ' alongside']\n",
            "The Wealth of Nations was published AFTER The Divine Comedy . → [' in', ' as', ' with', ' by', ' alongside']\n",
            "\n",
            "The Divine Comedy was published BEFORE Faust . → [' as', ' in', ' before', ' by', ' after']\n",
            "Faust was published AFTER The Divine Comedy . → [' in', ' as', ' by', ' with', ' after']\n",
            "\n",
            "The Divine Comedy was published BEFORE Brave New World . → [' in', ' by', ' at', ' as', ' on']\n",
            "Brave New World was published AFTER The Divine Comedy . → [' in', ' by', ' as', ' at', ' with']\n",
            "\n",
            "The Wealth of Nations was published BEFORE Brave New World . → [' in', ' by', ' at', ' on', ' as']\n",
            "Brave New World was published AFTER The Wealth of Nations . → [' in', ' by', ' as', ' at', ' on']\n",
            "\n",
            "The Wealth of Nations was published BEFORE Harry Potter . → [' in', ' as', ' before', ' after', ' with']\n",
            "Harry Potter was published AFTER The Wealth of Nations . → [' in', ' as', ' by', ' with', ' at']\n",
            "\n",
            "\n",
            "\n",
            "consistent_incorrect 3\n",
            "Faust was published AFTER To Kill a Mockingbird . → [' in', ' as', ' with', ' by', ' after']\n",
            "To Kill a Mockingbird was published BEFORE Faust . → [' in', ' by', ' August', ' on', ' April']\n",
            "\n",
            "Faust was published AFTER Harry Potter . → [' in', ' as', ' by', ' after', ' before']\n",
            "Harry Potter was published BEFORE Faust . → [' by', ' in', ' as', ' with', ' on']\n",
            "\n",
            "Brave New World was published AFTER To Kill a Mockingbird . → [' in', ' after', ' before', ' with', ' as']\n",
            "To Kill a Mockingbird was published BEFORE Brave New World . → [' in', ' by', ' at', ' as', ' on']\n",
            "\n",
            "\n",
            "\n",
            "inconsistent 20\n",
            "The Bible was published BEFORE The Divine Comedy . → [' as', ' in', ' with', ' alongside', ' by']\n",
            "The Divine Comedy was published BEFORE The Bible . → [' in', ' by', ' as', ' with', ' on']\n",
            "\n",
            "The Bible was published AFTER The Wealth of Nations . → [' as', ' in', ' by', ' with', ' titled']\n",
            "The Wealth of Nations was published AFTER The Bible . → [' in', ' by', ' on', ' at', ' as']\n",
            "\n",
            "The Bible was published AFTER Faust . → [' as', ' in', ' by', ' after', ' before']\n",
            "Faust was published AFTER The Bible . → [' in', ' by', ' on', ' at', ' as']\n",
            "\n",
            "The Bible was published BEFORE Moby Dick . → [' as', ' before', ' in', ' with', ' after']\n",
            "Moby Dick was published BEFORE The Bible . → [' in', ' as', ' by', ' with', ' before']\n",
            "\n",
            "The Bible was published AFTER Brave New World . → [' by', ' in', ' as', ' at', ' on']\n",
            "Brave New World was published AFTER The Bible . → [' in', ' by', ' on', ' at', ' as']\n",
            "\n",
            "The Bible was published BEFORE To Kill a Mockingbird . → [' as', ' in', ' with', ' before', ' after']\n",
            "To Kill a Mockingbird was published BEFORE The Bible . → [' in', ' by', ' on', ' at', ' as']\n",
            "\n",
            "The Bible was published BEFORE Harry Potter . → [' in', ' as', ' by', ' before', ' with']\n",
            "Harry Potter was published BEFORE The Bible . → [' in', ' by', ' as', ' with', ' before']\n",
            "\n",
            "The Divine Comedy was published BEFORE Moby Dick . → [' as', ' before', ' in', ' after', ' alongside']\n",
            "Moby Dick was published BEFORE The Divine Comedy . → [' as', ' in', ' with', ' alongside', ' before']\n",
            "\n",
            "The Divine Comedy was published BEFORE To Kill a Mockingbird . → [' in', ' as', ' before', ' with', ' after']\n",
            "To Kill a Mockingbird was published BEFORE The Divine Comedy . → [' in', ' as', ' by', ' at', ' with']\n",
            "\n",
            "The Divine Comedy was published BEFORE Harry Potter . → [' before', ' after', ' as', ' alongside', ' in']\n",
            "Harry Potter was published BEFORE The Divine Comedy . → [' in', ' as', ' by', ' with', ' before']\n",
            "\n",
            "The Wealth of Nations was published AFTER Faust . → [' by', ' in', ' on', ' as', ' at']\n",
            "Faust was published AFTER The Wealth of Nations . → [' in', ' as', ' by', ' at', ' on']\n",
            "\n",
            "The Wealth of Nations was published AFTER Moby Dick . → [' as', ' in', ' with', ' after', ' before']\n",
            "Moby Dick was published AFTER The Wealth of Nations . → [' in', ' as', ' with', ' by', ' alongside']\n",
            "\n",
            "The Wealth of Nations was published BEFORE To Kill a Mockingbird . → [' in', ' before', ' with', ' as', ' after']\n",
            "To Kill a Mockingbird was published BEFORE The Wealth of Nations . → [' in', ' by', ' as', ' at', ' with']\n",
            "\n",
            "Faust was published AFTER Moby Dick . → [' as', ' in', ' with', ' after', ' by']\n",
            "Moby Dick was published AFTER Faust . → [' by', ' in', ' as', ' with', ' under']\n",
            "\n",
            "Faust was published AFTER Brave New World . → [' in', ' by', ' at', ' on', ' as']\n",
            "Brave New World was published AFTER Faust . → [' by', ' in', ' on', ' as', ' with']\n",
            "\n",
            "Moby Dick was published BEFORE Brave New World . → [' in', ' by', ' as', ' with', ' at']\n",
            "Brave New World was published BEFORE Moby Dick . → [' as', ' in', ' before', ' after', ' alongside']\n",
            "\n",
            "Moby Dick was published BEFORE To Kill a Mockingbird . → [' as', ' before', ' in', ' after', ' with']\n",
            "To Kill a Mockingbird was published BEFORE Moby Dick . → [' as', ' in', ' before', ' after', ' with']\n",
            "\n",
            "Moby Dick was published BEFORE Harry Potter . → [' before', ' after', ' alongside', ' with', ' as']\n",
            "Harry Potter was published BEFORE Moby Dick . → [' before', ' after', ' as', ' alongside', ' in']\n",
            "\n",
            "Brave New World was published BEFORE Harry Potter . → [' before', ' alongside', ' after', ' in', ' with']\n",
            "Harry Potter was published BEFORE Brave New World . → [' by', ' in', ' as', ' at', ' with']\n",
            "\n",
            "To Kill a Mockingbird was published BEFORE Harry Potter . → [' before', ' after', ' alongside', ' as', ' with']\n",
            "Harry Potter was published BEFORE To Kill a Mockingbird . → [' before', ' as', ' after', ' in', ' with']\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1QvtwJWbB0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}