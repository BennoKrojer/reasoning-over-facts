{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "probeBERT_reverse.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-Fn1Qmh2w7Z",
        "colab_type": "text"
      },
      "source": [
        "This Notebook is intended to test relations that are symmetric or inverse.\n",
        "So to test this, we will ask \"a relation []?\" and then take the prediction p and ask \"p relation []?\". If the model has learnt that the relation is symmetric (or inverse), it should then say a, regardless of whether that is now correct or not.\n",
        "Unfortunately this approach only works properly for 1-to-1 relations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCg_nBdn1HVR",
        "colab_type": "code",
        "outputId": "bf7f282a-78a3-453c-81a4-96740e513979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 13.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 17.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 31.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=647ddea7b5aa42f0920af96a909a75295353a1e573fab57b01ee8578f6c3c4b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.7.0 transformers-2.9.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_q27yxX1YtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# subjects = ['Austria', 'Denmark', 'Switzerland', 'Ukraine', 'Belarus', 'Estonia', 'Afghanistan', 'Mexico', 'Egypt', 'Angola', 'Honduras', 'Panama', 'Turkey', 'Belgium', 'Mongolia', 'Hungary', 'Niger']\n",
        "subjects = ['to disagree', 'to admit', 'new', 'fresh', 'novel', 'job', 'occupation', 'price', 'cost', 'peak', 'summit', 'state', 'nation', 'country', 'land', 'earth', 'world', 'humanity', 'mankind', 'good', 'competent', 'situated', 'entire', 'path', 'big', 'great', 'large', 'real', 'actual', 'center', 'middle', 'travel', 'journey', 'trip', 'correct', 'proper', 'result', 'outcome', 'purpose', 'intention', 'region', 'subject', 'topic', 'field', 'discipline', 'home', 'category', 'type', 'class', 'black', 'dark', 'particular', 'movie', 'film', 'photo', 'picture']\n",
        "# subjects = ['absent', 'mountain', 'valley', 'present', 'dirt', 'house', 'careless', 'expensive', 'indefinite']\n",
        "lines = open('/content/drive/My Drive/tmp/antonyms').readlines()\n",
        "ant = []\n",
        "ant_verbs = []\n",
        "relation = 'is the same as'\n",
        "verb = 'is the same as'\n",
        "\n",
        "for line in lines:\n",
        "  if len(line) > 2 and ',' not in line:\n",
        "    line = line.replace('–','-')\n",
        "    first, second = line.strip().split(' - ')\n",
        "    # firsts = first.split(',')\n",
        "    # seconds = second.replace(' v','').split(',')\n",
        "    if ' v' in second:\n",
        "      second = second.replace(' v','')\n",
        "      first, second = 'to '+first, 'to '+second\n",
        "      first, second = f'[CLS] {first} {relation} to [MASK] . [SEP]', f'[CLS] {second} {relation} to [MASK] . [SEP]'\n",
        "      ant_verbs.append((first, second))\n",
        "    else:\n",
        "      first, second = f'[CLS] {first} {relation} [MASK] . [SEP]', f'[CLS] {second} {relation} [MASK] . [SEP]'\n",
        "      ant.append((first, second))\n",
        "\n",
        "# probes = [f'{subj} {relation}' for subj in subjects]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0JUhsiM5k4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "cased_model = True\n",
        "numb_predictions_displayed = 5\n",
        "ignore_self_reference_output = True # BERT tends to predict the subject again in many cases. This can be ignored."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSfjnyfN2edH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = open('capital_templates').readlines()\n",
        "probes = []\n",
        "for i in range(0,len(lines),2):\n",
        "  country = lines[i]\n",
        "  city = lines[i+1]\n",
        "  country = f'[CLS] {country.strip()} [MASK] . [SEP]'\n",
        "  city = f'[CLS] {city.strip()} [MASK] . [SEP]'\n",
        "  probes.append((country, city))\n",
        "probes[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni2lLPdm4vfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, tokenized_text):\n",
        "  masked_index = [i for i, x in enumerate(tokenized_text) if x == '[MASK]'][0]\n",
        "\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  segments_ids = [0]*len(tokenized_text)\n",
        "\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  # Predict all tokens\n",
        "  with torch.no_grad():\n",
        "      outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "      predictions = outputs[0][0][masked_index]\n",
        "  predicted_ids = torch.argsort(predictions, descending=True)[:numb_predictions_displayed]\n",
        "  predicted_tokens = tokenizer.convert_ids_to_tokens(list(predicted_ids))\n",
        "  return predicted_tokens\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xm2SbTa1ZMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM, RobertaForMaskedLM, RobertaTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# from: https://huggingface.co/transformers/quickstart.html#bert-example\n",
        "\n",
        "bert_model = 'bert-large-cased' if cased_model else 'bert-large-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
        "model = BertForMaskedLM.from_pretrained(bert_model)\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O2wZzH0cmFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = {'symmetric_correct': [], 'symmetric_incorrect': [],'asymmetric_correct':[], 'asymmetric_incorrect':[]}\n",
        "subj_idx = 1\n",
        "for probe in probes:\n",
        "  probe1, probe2 = probe[0], probe[1]\n",
        "  subj = probe1.split()[subj_idx]\n",
        "  obj = probe2.split()[subj_idx]\n",
        "  if len(tokenizer.tokenize(subj)) > 1 or len(tokenizer.tokenize(obj)) > 1:\n",
        "    print('INVALID: '+str(tokenizer.tokenize(subj)) + str(tokenizer.tokenize(obj)))\n",
        "    continue\n",
        "\n",
        "  tokenized_probe1 = tokenizer.tokenize(probe1)\n",
        "  tokenized_probe2 = tokenizer.tokenize(probe2)\n",
        "  print(tokenized_probe1)\n",
        "  print(tokenized_probe2)\n",
        "  predicted_tokens = predict(model, tokenized_probe1)\n",
        "\n",
        "  # determine first pred\n",
        "  first_pred = predicted_tokens[0] if predicted_tokens[0] != tokenized_probe1[subj_idx] else predicted_tokens[1]\n",
        "\n",
        "  reverse_probe_str = probe2.replace(obj, first_pred)\n",
        "  print(reverse_probe_str)\n",
        "  reverse_probe = tokenizer.tokenize(reverse_probe_str)\n",
        "  rev_predicted_tokens = predict(model, reverse_probe)\n",
        "  \n",
        "  rev_first_pred = rev_predicted_tokens[0] if rev_predicted_tokens[0] != reverse_probe[subj_idx] else rev_predicted_tokens[1]\n",
        "  \n",
        "  correct1 = first_pred == tokenized_probe2[subj_idx]\n",
        "  result = ((probe1, predicted_tokens), (reverse_probe_str, rev_predicted_tokens))\n",
        "  if rev_first_pred == tokenized_probe1[subj_idx]:\n",
        "    if correct1:\n",
        "      output['symmetric_correct'].append(result)\n",
        "    else:\n",
        "      output['symmetric_incorrect'].append(result)\n",
        "  else:\n",
        "    if correct1:\n",
        "      output['asymmetric_correct'].append(result)\n",
        "    else:\n",
        "      output['asymmetric_incorrect'].append(result)\n",
        "  if not correct1 and not rev_first_pred == tokenized_probe1[1]:\n",
        "    predicted_tokens2 = predict(model, tokenized_probe2)\n",
        "    first_pred2 = predicted_tokens2[0] if predicted_tokens2[0] != tokenized_probe2[subj_idx] else predicted_tokens2[1]\n",
        "    reverse_probe_str2 = probe1.replace(subj, first_pred2)\n",
        "    reverse_probe2 = tokenizer.tokenize(reverse_probe_str2)\n",
        "    rev_predicted_tokens2 = predict(model, reverse_probe2)\n",
        "    rev_first_pred2 = rev_predicted_tokens2[0] if rev_predicted_tokens2[0] != reverse_probe2[subj_idx] else rev_predicted_tokens2[1]\n",
        "    correct2 = first_pred2 == tokenized_probe1[subj_idx]\n",
        "    symmetric2 = rev_first_pred2 == tokenized_probe2[subj_idx]\n",
        "    result = ((probe2, predicted_tokens2), (reverse_probe_str2, rev_predicted_tokens2))\n",
        "    if correct2 and not symmetric2:\n",
        "      output['asymmetric_correct'].append(result)\n",
        "    if correct2 and symmetric2:\n",
        "      print(\"IMPOSSIBLE\"+str(result))\n",
        "    if not correct2 and symmetric2:\n",
        "      output['symmetric_incorrect'].append(result)\n",
        "    if not correct2 and not symmetric2:\n",
        "      output['asymmetric_incorrect'].append(result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzRLLNs763kG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for key, val in output.items():\n",
        "  print(key + ' ' + str(len(val)))\n",
        "  for (probe, pred), (rev_probe, rev_pred) in val:\n",
        "    print(f'{probe} → {pred}')\n",
        "    print(f'{rev_probe} → {rev_pred}\\n')\n",
        "\n",
        "  print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMnPYbcYPH7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}