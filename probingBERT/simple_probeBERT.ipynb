{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "probeBERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcOSzM4ddYwH",
        "colab_type": "text"
      },
      "source": [
        "This is a simple Notebook to probe BERT for facts in the form \"Subject relation [clozed object]\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnH7YtxQM7Km",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryq6wcjZeelZ",
        "colab_type": "text"
      },
      "source": [
        "Let's first set up our probes and some detailed settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQwkcDpP6fsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subjects = ['France', 'Paris', 'Bonn', 'Mount Everest']\n",
        "relation = 'is located in'\n",
        "# answers = ['Europe', 'France', 'Germany', ['Nepal', 'China'], '']\n",
        "answers = ['']*len(subjects)\n",
        "probes = [f'{subj} {relation}' for subj in subjects]\n",
        "cased_model = True\n",
        "numb_predictions_displayed = 5\n",
        "ignore_self_reference_output = True # BERT tends to predict the subject again in many cases. This can be ignored."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ru4XQlreoLg",
        "colab_type": "text"
      },
      "source": [
        "Let's probe!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRNTd42zopnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM, RobertaForMaskedLM, RobertaTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# from: https://huggingface.co/transformers/quickstart.html#bert-example\n",
        "\n",
        "bert_model = 'bert-large-cased' if cased_model else 'bert-large-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
        "model = BertForMaskedLM.from_pretrained(bert_model)\n",
        "model.eval()\n",
        "\n",
        "output = {'correct': [], 'false': [], 'undefined': []}\n",
        "for probe, answer in zip(probes, answers):\n",
        "  text = f'[CLS] {probe} [MASK] . [SEP]'\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  masked_index = [i for i, x in enumerate(tokenized_text) if x == '[MASK]'][0]\n",
        "  print(f'TOKENIZED TEXT: {tokenized_text}')\n",
        "\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  segments_ids = [0]*len(tokenized_text)\n",
        "\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  # Predict all tokens\n",
        "  with torch.no_grad():\n",
        "      outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "      predictions = outputs[0][0][masked_index]\n",
        "  predicted_ids = torch.argsort(predictions, descending=True)[:numb_predictions_displayed]\n",
        "  predicted_tokens = tokenizer.convert_ids_to_tokens(list(predicted_ids))\n",
        "  if answer:\n",
        "    if isinstance(answer, str):\n",
        "      answer = [answer]\n",
        "    first_pred = predicted_tokens[0]\n",
        "    if first_pred == tokenized_text[1]:\n",
        "      first_pred = predicted_tokens[1]\n",
        "    if first_pred in answer:\n",
        "      output['correct'].append((probe, predicted_tokens))\n",
        "    else:\n",
        "      output['false'].append((probe, predicted_tokens))\n",
        "  else:\n",
        "    output['undefined'].append((probe, predicted_tokens))\n",
        "\n",
        "for key, val in output.items():\n",
        "  print(key)\n",
        "  for probe, predicted_tokens in val:\n",
        "    print(f'{probe} [MASK] â†’ {predicted_tokens}')\n",
        "  print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOB0g5qggsrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}