Dec 1, 23:00:
applying new code to old data didn't work
(used layers=4 specified in code, no config file, therefore default vocab_size=30522)
(used torch 1.5, transformers 2.5.1)

Dec 2, 9:00:
same as above but now additionally with explicit vocab_size=6006 in code, did it for sampled_500attri
eval_acc = 0.27 after ~1800 epochs

Dec 3, 13:50:
started training with deprecated_run_language_modeling.py & deprecated_train_smaller.py
with the config files under negation/vocab/sampled_500attri/
I trained with with vocab=30522 and result is after 1200 epochs:
Eval_Accuracy_intersected = 0.01533

Dec 3, 21:25:
started training with deprecated_run_language_modeling.py & deprecated_train_smaller.py
with the config files under negation/vocab/sampled_500attri/
I trained with with vocab=6006 and result is after 1700 epochs:
Eval_Accuracy_intersected = 0.2556610664718773

Dec 5, 9:00:
new code is equal to old code. let's use new code.
vocab_size=6506, data=neg_sampled, question: does it get the reported acc? or did we back then simply not increase vocab size?